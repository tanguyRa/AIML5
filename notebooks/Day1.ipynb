{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About me\n",
    "![Flexper, innovation on demand](img/flexper_logo_large.jpg)\n",
    "\n",
    "My name is **Tanguy Racinet** and I currently work at **Flexper**, a technological accelerator for startups.\n",
    "\n",
    "### How to reach me\n",
    "**mail**: tanguy@flexper.fr\n",
    "\n",
    "**linkedin**: https://www.linkedin.com/in/tanguyracinet/\n",
    "\n",
    "### Examples of projects I worked on\n",
    " * Movie rentals estimation and expected return on investment for movie streaming platform\n",
    " * Recommendation system for playlist generation in a streaming music platform \n",
    " * Vehicle expected return and pricing over time in car renting service\n",
    " * Pollution evolution in air traffic study for the DGAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics covered in the course - what you should know about by the end\n",
    "1. Generalities about Machine Learning\n",
    "2. Classification algorithms\n",
    "3. Recommender systems\n",
    "4. Linear regression\n",
    "5. Decision trees\n",
    "\n",
    "# Course overview - what we're actually going to do\n",
    "1. Day 1\n",
    "    * open Q&A\n",
    "    * Practice 1 - Generalities about Machine Learning\n",
    "    * Practice 2 - Classification algorithms\n",
    "2. Day 2\n",
    "    * open Q&A\n",
    "    * Practice - Recommender systems\n",
    "3. Day 3\n",
    "    * open Q&A\n",
    "    * Practice 1 - Decision trees\n",
    "    * Practice 2 - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning concepts\n",
    "\n",
    "Let's take a concrete example to better understand the different concepts and definition that you will require when developping a classification engine as a Machine Learning engineer.\n",
    "\n",
    "### Your first day at the fair\n",
    "You just arrive to the slaughterfest in super hero city, the biggest showdown of the year for powered people. You have been tasked by city officials to develop a model that can automatically identify superheros against supervilains in order to hopefully mitigate casualties.\n",
    "\n",
    "You quickly gather your notes on all the powered people you've heard of and assemble your very own dataset. You still have a lot to do in order to get your data ready before you can train your first model.\n",
    "![dataset: super people]()\n",
    "\n",
    "### Data splitting: Training, validation, test\n",
    "You know that you should **never** evaluate your model on the data you used to train it since this is the best way to overfit your model, therefore you start by dividing your data in 2 distinct sets:\n",
    " * the *training set* to train your model\n",
    " * the *test set* to evaluate your trained model and check how well it generalise to unknown super persons.\n",
    "![data splitting]()\n",
    " \n",
    "After careful consideration, you end up deciding against splitting your test set in **test** and **validation** given how small your dataset is. \n",
    "\n",
    "### Cross validation\n",
    "Since you plan on using k-fold cross validation to automatically split your training set anyway, you should be covered. You will be able to train your model on k-1 splits of your training set k times to maximize your learning and find a model with good generalisation capabilities.\n",
    "![cross validation with k-fold]()\n",
    "\n",
    "\n",
    "### Encoding\n",
    "Despite being a proficient Machine Learning engineer, you're still human and you used words instead of sweet hard numbers when creating your dataset. You decide to quickly review in your head the different encoding techniques you could use on your data:\n",
    " * Label encoding for graduated variables\n",
    " * One hot encoding for independant variables\n",
    " * Cyclical encoding for... Well cyclical variables...\n",
    "![encoding]()\n",
    "\n",
    "### Missing value inputation\n",
    "Another problem with your dataset is that because of your memory problems, some data is missing from your dataset. That black and white picture you collected isn't helping you when deciding what color is his mask. You're now stuck trying to figure out how to fill in for the missing value.\n",
    "![data imput]()\n",
    "\n",
    "### Confusion matrix\n",
    "Now that you're done with data processing, you can finally train your model. So let's get cracking and build your confusion matrix to evaluate your model performance:\n",
    "![confusion matrix with log reg curve]()\n",
    "\n",
    "### ROC curve\n",
    "Alright, you've got a somewhat functional model. But now that you're getting close to the end, you're receiving offers from both the superheros and the supervilains who are all trying to outbid each other for your algorithm.\n",
    "You now have the power to sell it to either side so you'de better figure out who would profit most from it. The ROC curve is the perfect tool for you to illustrate your algorithm constraints.\n",
    "![roc curve construction with confusion matrices]()\n",
    "If you are intending on providing your solution to the superheros, they are probably interested in being able to identify supervilains to arrest them and send them to jail. But it would be problematic if they were to send other superheroes to jail because taht would reduce their forces and let's face it... That wouldn't be very heroic of them to condemn innocent people.\n",
    "\n",
    "On the other hand, vilains would probably not be as prudent with it and might not have an issue with taking out a few of their own, just in case they might be vigilantes.\n",
    "The ROC curve will help you identify the best threshold or hyper parameters for your algorithm, depending on your problem constraints.\n",
    "\n",
    "### Area Under the Curve\n",
    "After selling your first algorithm for a very nice profit, you now have enough funds to investigate other potential algorithm that might be even more efficient that your first attempt.\n",
    "![AUC for different models]()\n",
    "You compute the AUC for all the different models you trained in order to identify the ultimate model that will definitely help you make a difference in super hero city. (and/or get filthy rich, that's your story after all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms on Heart Diseases\n",
    "Download the [data](https://canvas.supinfo.com/courses/85/files/7110/download?wrap=1) directly from [UCI]()'s website and save it under ./data of your current directory on your host. (/home/jovyan/data in the docker container)\n",
    "## Importing required libraries\n",
    "Always put your imports at the top, it will greatly simplify your work when turning your notebooks into actual python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting pandas_profiling\n",
      "  Downloading pandas-profiling-2.5.0.tar.gz (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (1.4.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (0.25.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (3.1.3)\n",
      "Collecting confuse==1.0.0\n",
      "  Downloading confuse-1.0.0.tar.gz (32 kB)\n",
      "Collecting jinja2==2.11.1\n",
      "  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 25.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting visions==0.2.2\n",
      "  Downloading visions-0.2.2.tar.gz (27 kB)\n",
      "Collecting htmlmin==0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "Collecting missingno==0.4.2\n",
      "  Downloading missingno-0.4.2-py3-none-any.whl (9.7 kB)\n",
      "Collecting phik==0.9.9\n",
      "  Downloading phik-0.9.9-py3-none-any.whl (607 kB)\n",
      "\u001b[K     |████████████████████████████████| 607 kB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astropy>=3.2.3\n",
      "  Downloading astropy-4.0-cp37-cp37m-manylinux1_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tangled-up-in-unicode==0.0.3\n",
      "  Downloading tangled_up_in_unicode-0.0.3.tar.gz (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 27.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm==4.42.0 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (4.42.0)\n",
      "Collecting kaggle==1.5.6\n",
      "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets==7.5.1 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (7.5.1)\n",
      "Requirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.7/site-packages (from pandas_profiling) (2.22.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3->pandas_profiling) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==0.25.3->pandas_profiling) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->pandas_profiling) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->pandas_profiling) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->pandas_profiling) (2.4.6)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from confuse==1.0.0->pandas_profiling) (5.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2==2.11.1->pandas_profiling) (1.1.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from visions==0.2.2->pandas_profiling) (2.4)\n",
      "Collecting attr\n",
      "  Downloading attr-0.3.1.tar.gz (1.7 kB)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (from missingno==0.4.2->pandas_profiling) (0.9.0)\n",
      "Collecting pytest-pylint>=0.13.0\n",
      "  Downloading pytest_pylint-0.15.0-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /opt/conda/lib/python3.7/site-packages (from phik==0.9.9->pandas_profiling) (0.14.1)\n",
      "Requirement already satisfied: jupyter-client>=5.2.3 in /opt/conda/lib/python3.7/site-packages (from phik==0.9.9->pandas_profiling) (5.3.4)\n",
      "Collecting pytest>=4.0.2\n",
      "  Downloading pytest-5.3.5-py3-none-any.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbconvert>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from phik==0.9.9->pandas_profiling) (5.6.1)\n",
      "Requirement already satisfied: numba>=0.38.1 in /opt/conda/lib/python3.7/site-packages (from phik==0.9.9->pandas_profiling) (0.48.0)\n",
      "Collecting urllib3<1.25,>=1.21.1\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle==1.5.6->pandas_profiling) (1.14.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle==1.5.6->pandas_profiling) (2019.11.28)\n",
      "Collecting python-slugify\n",
      "  Downloading python-slugify-4.0.0.tar.gz (8.8 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.5.1->pandas_profiling) (7.11.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.5.1->pandas_profiling) (5.0.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.5.1->pandas_profiling) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.5.1->pandas_profiling) (5.1.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.5.1->pandas_profiling) (4.3.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->pandas_profiling) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.22.0->pandas_profiling) (2.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.3->pandas_profiling) (45.1.0.post20200119)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->visions==0.2.2->pandas_profiling) (4.4.1)\n",
      "Collecting pylint>=2.0.0\n",
      "  Downloading pylint-2.4.4-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tornado>=4.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=5.2.3->phik==0.9.9->pandas_profiling) (6.0.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=5.2.3->phik==0.9.9->pandas_profiling) (4.6.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=5.2.3->phik==0.9.9->pandas_profiling) (18.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=4.0.2->phik==0.9.9->pandas_profiling) (19.3.0)\n",
      "Collecting py>=1.5.0\n",
      "  Downloading py-1.8.1-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest>=4.0.2->phik==0.9.9->pandas_profiling) (1.5.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest>=4.0.2->phik==0.9.9->pandas_profiling) (0.1.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest>=4.0.2->phik==0.9.9->pandas_profiling) (20.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=4.0.2->phik==0.9.9->pandas_profiling) (8.2.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (0.4.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (3.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (0.6.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (2.5.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (1.4.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (0.8.4)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.38.1->phik==0.9.9->pandas_profiling) (0.31.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (0.16.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (3.0.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (4.8.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->pandas_profiling) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->pandas_profiling) (3.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.5.1->pandas_profiling) (6.0.3)\n",
      "Collecting astroid<2.4,>=2.3.0\n",
      "  Downloading astroid-2.3.3-py3-none-any.whl (205 kB)\n",
      "\u001b[K     |████████████████████████████████| 205 kB 24.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting isort<5,>=4.2.5\n",
      "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=4.0.2->phik==0.9.9->pandas_profiling) (2.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5.3.1->phik==0.9.9->pandas_profiling) (0.5.1)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (0.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.5.1->pandas_profiling) (0.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.5.1->pandas_profiling) (0.15.7)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->pandas_profiling) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->pandas_profiling) (0.7.1)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->pandas_profiling) (1.5.0)\n",
      "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
      "  Downloading typed_ast-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt==1.11.*\n",
      "  Downloading wrapt-1.11.2.tar.gz (27 kB)\n",
      "Collecting lazy-object-proxy==1.4.*\n",
      "  Downloading lazy_object_proxy-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pandas-profiling, confuse, visions, htmlmin, tangled-up-in-unicode, kaggle, attr, python-slugify, wrapt\n",
      "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandas-profiling: filename=pandas_profiling-2.5.0-py2.py3-none-any.whl size=241330 sha256=11a54ff046e0c16f57a3e96594a86f7d80fc5db43160da5e3aa41cdd9bec54c3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/9c/ba/e7/56357258045b5f679399fdef61c40c819dc03b22bba3880795\n",
      "  Building wheel for confuse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for confuse: filename=confuse-1.0.0-py3-none-any.whl size=17487 sha256=85680c3008709c75e2be490f9c250ff81a5b435a6b3c8a26ae4abcce1631c1b3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/91/12/f3/c3b142bda8c6aaa03c14323c7c6bbe7e9eea58101ca1f56260\n",
      "  Building wheel for visions (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visions: filename=visions-0.2.2-py3-none-any.whl size=53059 sha256=cc0f8710b9eb15d911a6ce2491659a3ac2355ab62e733f5dba830212fefbdc4c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/25/6e/dc/2d379b18ee47c7c2e12969eaaa9a57a8ca24c3f342df840a4e\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27085 sha256=2b5b263f058d2adb998c30de4870e2c621d2c4e5e99acb8d7d3c919d65d322ca\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
      "  Building wheel for tangled-up-in-unicode (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tangled-up-in-unicode: filename=tangled_up_in_unicode-0.0.3-py3-none-any.whl size=1554153 sha256=81fb6c99daada0b1130dda4ade73528d276bc90e5a6b94966de7e234e7505001\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/93/48/a6/a316140da6c618916f1ceaef69948e2e65a08bd18f7ea3bfa8\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72858 sha256=09c06acd5e9cf4d91b76fce818556f4bc7b3f30a6c637b80d832a5b94f36df36\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b\n",
      "  Building wheel for attr (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for attr: filename=attr-0.3.1-py3-none-any.whl size=2457 sha256=e2350270c16121d05c200bc84fa1a230d7e3c3e486a5ce3081b7191e5f14e9e2\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3b/5d/58/41fbe92f47031641008bd8559ee89e58bf0f123f9c18dea1cb\n",
      "  Building wheel for python-slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-slugify: filename=python_slugify-4.0.0-py2.py3-none-any.whl size=5487 sha256=06434d9e1908c675e561891572f0fd68bf90b097eb309d70b22109037a354545\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7c/26/30/5f3d95da00fe94d0c4a5ec5b4ffd2e1ae18545f5fa61752e52\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.11.2-cp37-cp37m-linux_x86_64.whl size=70700 sha256=bbabbab520d77345d3be1da820267b363d8fb12ac0dd9918d7697ce2d9103242\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/23/5f/62/304b411f20be41821465a82bc98baabc5e68c3cdd1eb99db71\n",
      "Successfully built pandas-profiling confuse visions htmlmin tangled-up-in-unicode kaggle attr python-slugify wrapt\n",
      "Installing collected packages: confuse, jinja2, tangled-up-in-unicode, attr, visions, htmlmin, missingno, py, pluggy, pytest, typed-ast, wrapt, lazy-object-proxy, astroid, isort, mccabe, pylint, pytest-pylint, phik, astropy, urllib3, text-unidecode, python-slugify, kaggle, pandas-profiling\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.0\n",
      "    Uninstalling Jinja2-2.11.0:\n",
      "      Successfully uninstalled Jinja2-2.11.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.7\n",
      "    Uninstalling urllib3-1.25.7:\n",
      "      Successfully uninstalled urllib3-1.25.7\n",
      "Successfully installed astroid-2.3.3 astropy-4.0 attr-0.3.1 confuse-1.0.0 htmlmin-0.1.12 isort-4.3.21 jinja2-2.11.1 kaggle-1.5.6 lazy-object-proxy-1.4.3 mccabe-0.6.1 missingno-0.4.2 pandas-profiling-2.5.0 phik-0.9.9 pluggy-0.13.1 py-1.8.1 pylint-2.4.4 pytest-5.3.5 pytest-pylint-0.15.0 python-slugify-4.0.0 tangled-up-in-unicode-0.0.3 text-unidecode-1.3 typed-ast-1.4.1 urllib3-1.24.3 visions-0.2.2 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation:\n",
    "import pandas as pd\n",
    "\n",
    "# Data exploration:\n",
    "!pip install pandas_profiling\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# scikit learn ML models\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# cross validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = pd.read_csv('../data/HeartDiseaseUCI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "### What you need to understand\n",
    "A very good tutorial on data exploration: [kaggle tutorial](https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis/notebook)\n",
    "\n",
    "### And the quick win version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b596f08c87a045c3956eeff14bf29582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='variables', max=15.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc9af3d9cdf482f88704077ea749aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='correlations', max=6.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b05c8243cf49a7bca5672e2a25f9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='interactions [continuous]', max=49.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ee11ed2f4941a5b4172fb242ca6f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='table', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2ff226407a4580b7e04fa8066d66bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='missing', max=4.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b51ade062ae4aeaadf728187c4ddb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='warnings', max=3.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197414b284cb44f4a1200cf492542587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='package', max=1.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d031e7e004ebdaaf1550e04b953f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='build report structure', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = '../data'\n",
    "\n",
    "# Get raw bookings file statistic report\n",
    "data_report = pp.ProfileReport(heart_disease_df)\n",
    "data_report.to_file(output_file='{}/heart_disease_report.html'.format(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0609089657114a29910943d338a908bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(value='Number of va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Report generated with <a href=\"https://github.com/pandas-profiling/pandas-profiling\">pandas-profiling</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_report.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "We replace the varying degree of disease with a binary classification: healthy/sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  \\\n",
       "0           1   63    1   1       145   233    1        2      150      0   \n",
       "1           2   67    1   4       160   286    0        2      108      1   \n",
       "\n",
       "   oldpeak  slope   ca  thal      num  \n",
       "0      2.3      3  0.0   6.0  healthy  \n",
       "1      1.5      2  3.0   3.0     sick  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_num_df = heart_disease_df.copy()\n",
    "\n",
    "categorical_num_df['num'] = categorical_num_df['num'].apply(\n",
    "    lambda x: 'healthy' if x == 0 else 'sick'\n",
    ")\n",
    "categorical_num_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because scikit learn models only work with numerical values, we need to replace our *healthy* and *sick* labels. There are two potnetial of turning this into numerical values:\n",
    " * Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  \\\n",
       "0           1   63    1   1       145   233    1        2      150      0   \n",
       "1           2   67    1   4       160   286    0        2      108      1   \n",
       "\n",
       "   oldpeak  slope   ca  thal  num  \n",
       "0      2.3      3  0.0   6.0    0  \n",
       "1      1.5      2  3.0   3.0    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoded_df = categorical_num_df.copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(label_encoded_df['num'])\n",
    "label_encoded_df['num'] = le.transform(label_encoded_df['num'])\n",
    "\n",
    "label_encoded_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num_healthy</th>\n",
       "      <th>num_sick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  \\\n",
       "0           1   63    1   1       145   233    1        2      150      0   \n",
       "1           2   67    1   4       160   286    0        2      108      1   \n",
       "\n",
       "   oldpeak  slope   ca  thal  num_healthy  num_sick  \n",
       "0      2.3      3  0.0   6.0            1         0  \n",
       "1      1.5      2  3.0   3.0            0         1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(categorical_num_df).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick win:\n",
    "One-liner that we could have used directly, after understanding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  \\\n",
       "0           1   63    1   1       145   233    1        2      150      0   \n",
       "1           2   67    1   4       160   286    0        2      108      1   \n",
       "\n",
       "   oldpeak  slope   ca  thal  num  \n",
       "0      2.3      3  0.0   6.0    0  \n",
       "1      1.5      2  3.0   3.0    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease_df['num'] = heart_disease_df['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "heart_disease_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease_df[[ \n",
    "    col for col in heart_disease_df.columns if col not in ['num', 'Unnamed: 0']\n",
    "]]\n",
    "y = heart_disease_df['num']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_train: (212, 13)', 'y_train: (212,)', 'X_test: (91, 13)', 'y_test: (91,)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "211   38    1   1       120   231    0        0      182      1      3.8   \n",
       "67    54    1   3       150   232    0        2      165      0      1.6   \n",
       "25    50    0   3       120   219    0        0      158      0      1.6   \n",
       "196   69    1   1       160   234    1        2      131      0      0.1   \n",
       "175   57    1   4       152   274    0        0       88      1      1.2   \n",
       "\n",
       "     slope   ca  thal  \n",
       "211      2  0.0   7.0  \n",
       "67       1  0.0   7.0  \n",
       "25       2  0.0   3.0  \n",
       "196      2  1.0   3.0  \n",
       "175      2  1.0   7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "211    1\n",
       "67     0\n",
       "25     0\n",
       "196    0\n",
       "175    1\n",
       "Name: num, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    [\n",
    "        'X_train: {}'.format(X_train.shape), 'y_train: {}'.format(y_train.shape), \n",
    "        'X_test: {}'.format(X_test.shape), 'y_test: {}'.format(y_test.shape)\n",
    "    ],\n",
    "    X_train.tail(),\n",
    "    y_train.tail()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "Pipelines in scikit learn are a very important concept. They encapsulate a number of **transformers** and **estimators**.\n",
    "\n",
    "**Transformers** apply a function over your data to modify it, commonly used to apply preprocessing steps over the data before training a model.\n",
    "\n",
    "**Estimators** apply a ML model to make predictions.\n",
    "\n",
    "These are the steps we'll take when [preprocessing our data with scikit learn](https://scikit-learn.org/stable/modules/preprocessing.html) before we can apply our model:\n",
    " * [Fill in missing values](https://scikit-learn.org/stable/modules/impute.html). We will use the [Simple Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    " * Scale and center the data with the [Standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to reduce feature range impact for enclidian distances. \n",
    " * We can also conduct a [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the number of dimensions and extract meaningful informations from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classification model: [K-Neirest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "Simple pipeline example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 4) Processing imputer, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 4) Processing pca, total=   0.0s\n",
      "[Pipeline] .... (step 4 of 4) Processing knn_classifier, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8021978021978022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe_example = Pipeline(\n",
    "    steps = [\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=12)),\n",
    "        ('knn_classifier', KNeighborsClassifier(n_neighbors=10))\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "knn_pipe_example.fit(X_train, y_train)\n",
    "knn_pipe_example.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the same with python *list* and *dict* comprehension to reuse the same preprocessing steps for every model we'll test, along with a **cross_validate** utility function to perform grid search with multiple hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_steps = [\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=12))\n",
    "]\n",
    "preprocessing_params = {\n",
    "    'imputer__strategy': ['mean', 'median'],\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'scaler__with_std': [True, False],\n",
    "    'pca__n_components': [11, 12, 13]\n",
    "}\n",
    "\n",
    "def cross_validate(model, params):\n",
    "    # create a gridsearch of the pipeline, the find the best hyper parameters\n",
    "    grid_param = { **preprocessing_params, **params }\n",
    "    gridsearch = GridSearchCV(model, grid_param, cv=5, verbose=1, n_jobs=-1)\n",
    "    gridsearch_result = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "    display(gridsearch_result.best_estimator_)\n",
    "    display('Best model accuracy over previously unseen data: {}'.format(\n",
    "        gridsearch_result.score(X_test, y_test)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=12,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 KNeighborsClassifier(algorithm='brute', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=10, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8021978021978022'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_pipe = Pipeline(\n",
    "    preprocessing_steps + [('classifier', KNeighborsClassifier())]\n",
    ")\n",
    "knn_params = {\n",
    "    'classifier__n_neighbors': [5, 10, 15, 20],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['brute', 'ball_tree']\n",
    "}\n",
    "\n",
    "cross_validate(knn_pipe, knn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classification model: [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=12,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LinearSVC(C=0.5, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=0,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8131868131868132'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_pipe = Pipeline(\n",
    "    steps = preprocessing_steps + [('classifier', LinearSVC(random_state=0))]\n",
    ")\n",
    "svm_params = {\n",
    "    'classifier__C': [0.5, 1, 1.5],\n",
    "    'classifier__loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "cross_validate(svm_pipe, svm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classification model: [Linear Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=False)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=11,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LinearDiscriminantAnalysis(n_components=None, priors=None,\n",
       "                                            shrinkage=None, solver='svd',\n",
       "                                            store_covariance=False,\n",
       "                                            tol=0.0001))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8131868131868132'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_pipe = Pipeline(\n",
    "    steps = preprocessing_steps + [('classifier', LinearDiscriminantAnalysis())]\n",
    ")\n",
    "lda_params = {\n",
    "    'classifier__solver': ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "cross_validate(lda_pipe, lda_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classification model: [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=False)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=11,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8571428571428571'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnb_pipe = Pipeline(steps = preprocessing_steps + [('classifier', GaussianNB())])\n",
    "gnb_params = {}\n",
    "\n",
    "cross_validate(gnb_pipe, gnb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classification model: [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 949 out of 960 | elapsed:    4.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=12,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=0.5, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8131868131868132'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_pipe = Pipeline(preprocessing_steps + [('classifier', LogisticRegression())])\n",
    "lr_params = {\n",
    "    'classifier__C': [0.1, 0.5, 1, 1.5],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "cross_validate(lr_pipe, lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out different setups: [Cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) over multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 816 candidates, totalling 8160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2420 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6420 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 8160 out of 8160 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=False)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=11,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Best model accuracy over previously unseen data: 0.8131868131868132'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline(preprocessing_steps + [('classifier', LogisticRegression())])\n",
    "\n",
    "# Create list with candidate learning algorithms and their hyperparameters\n",
    "grid_param = [\n",
    "    { \n",
    "        'classifier': [KNeighborsClassifier()], \n",
    "        **preprocessing_params, \n",
    "        **knn_params \n",
    "    },\n",
    "    { \n",
    "        'classifier': [LinearSVC()], \n",
    "        **preprocessing_params, \n",
    "        **svm_params \n",
    "    },\n",
    "    { \n",
    "        'classifier': [LinearDiscriminantAnalysis()], \n",
    "        **preprocessing_params, \n",
    "        **lda_params\n",
    "    },\n",
    "    { \n",
    "        'classifier': [GaussianNB()], \n",
    "        **preprocessing_params, \n",
    "        **gnb_params \n",
    "    },\n",
    "    { \n",
    "        'classifier': [LogisticRegression()], \n",
    "        **preprocessing_params, \n",
    "        **lr_params \n",
    "    }\n",
    "]\n",
    "\n",
    "# create a gridsearch of the pipeline, then fit the best model\n",
    "gridsearch = GridSearchCV(pipe, grid_param, cv=10, verbose=1, n_jobs=-1)\n",
    "gridsearch_result = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "display(gridsearch_result.best_estimator_)\n",
    "display('Best model accuracy over previously unseen data: {}'.format(\n",
    "    gridsearch_result.score(X_test, y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
